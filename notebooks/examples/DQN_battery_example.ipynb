{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main aim of this notebook is to demo the low level energy_py API.  All of the functionality exposed here is wrapped into a higher level API that can be accessed by\n",
    "\n",
    "```bash\n",
    "$ cd energy_py/experiments\n",
    "\n",
    "$ python experiment.py example dqn\n",
    "```\n",
    "\n",
    "This higher level API wraps more functionality than is exposed int his example - i.e. generating log files and writing data to TensorBoard.  For the scope of this low level example, we will just use data available locally - episode rewards in the `Runner` class and data for the last episode - the `info` dictionary.\n",
    "\n",
    "This notebook also demonstrates the ability of a DQN agent to learn to optimize simplfied electric battery storage.\n",
    "\n",
    "This example involves a constant and repetitive electricity price profile, combined with a perfect forecast. The agent has both the ability to memorize this profile and lives in a near Markov environment.  More interesting work randomly samples different price rollouts and uses realistic forecasts.  \n",
    "\n",
    "A real world application of using reinforcement learning to control a battery would have to deal with both a variable price profile and a non-Markov understanding of what the price profile would do in the future.  It could also involve additional reward signals, such as payments from fast frequency response needed to be balanced against price arbitrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/anaconda3/envs/energy_py/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/adam/anaconda3/envs/energy_py/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import energy_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  define a total number of steps for the experiment to run\n",
    "TOTAL_STEPS = 500000\n",
    "\n",
    "#  to setup the agent we use a dictionary\n",
    "#  a dictionary allows us to eaisly save the config to csv if we want\n",
    "agent_config = {\n",
    "    'discount': 0.97,                 #  the discount rate\n",
    "    'tau': 0.001,                     #  parameter that controls the copying of weights from online to target network\n",
    "    'total_steps': TOTAL_STEPS,   \n",
    "    'batch_size': 32,                 #  size of the minibatches used for learning\n",
    "    'layers': (50, 50),               #  structure of the neural network used to approximate Q(s,a)\n",
    "    'learning_rate': 0.0001,          #  controls the stength of weight updates during learning       \n",
    "    'epsilon_decay_fraction': 0.3,    #  a fraction as % of total steps where epsilon decayed from 1.0 to 0.1\n",
    "    'memory_fraction': 0.4,           #  the size of the replay memory as a % of total steps\n",
    "    'memory_type': 'deque',           #  the replay memory implementation we want\n",
    "               }\n",
    "\n",
    "#  keep all of the BatteryEnv variables (episode length, efficiency etc) at their defaults\n",
    "#  we just need to let our env know where our state.csv and observation.csv are (data_path)\n",
    "env = energy_py.make_env('battery')\n",
    "\n",
    "#  set seeds for reproducibility\n",
    "env.seed(42)\n",
    "\n",
    "def print_time():\n",
    "    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-02 02:19:55\n",
      "WARNING:tensorflow:From /Users/adam/anaconda3/envs/energy_py/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/adam/anaconda3/envs/energy_py/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1 0.40 % rew [[-884.40927193]]\n",
      "ep 11 4.43 % rew [[-1301.93839474]]\n",
      "ep 21 8.46 % rew [[-848.06097368]]\n",
      "ep 31 12.49 % rew [[-757.40789474]]\n",
      "ep 41 16.52 % rew [[-865.33085502]]\n",
      "ep 51 20.55 % rew [[-810.97214035]]\n"
     ]
    }
   ],
   "source": [
    "print_time()\n",
    "#  reset the graph (without this nb needs to be restart each time)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#  initialize Tensorflow machinery\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #  Runner is a class that helps us with experiments - tracking rewards, writing environment info to csv and managing TensorBoard\n",
    "    #  in this notebook we just use it to track rewards for us\n",
    "    runner = energy_py.Runner(\n",
    "        sess,  \n",
    "        {'tb_rl': './tb_rl',\n",
    "         'ep_rewards': './rewards.csv'}\n",
    "    )\n",
    "    \n",
    "    #  add the tf session and the environment to the agent config dictionary\n",
    "    #  and initialize the agent\n",
    "    agent_config['sess'] = sess\n",
    "    agent_config['env'] = env\n",
    "    agent = energy_py.make_agent(agent_id='dqn', **agent_config)\n",
    "    \n",
    "    #  initial values for the step and episode number\n",
    "    step, episode = 0, 0\n",
    "\n",
    "    #  outer while loop runs through multiple episodes\n",
    "    while step < TOTAL_STEPS:\n",
    "        episode += 1\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        \n",
    "        #  inner while loop runs through a single episode\n",
    "        while not done:\n",
    "            step += 1\n",
    "            #  select an action\n",
    "            action = agent.act(observation)\n",
    "            \n",
    "            #  take one step through the environment\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            #  store the experience\n",
    "            agent.remember(observation, action, reward,\n",
    "                           next_observation, done)\n",
    "            \n",
    "            #  moving to the next time step\n",
    "            observation = next_observation\n",
    "            #  saving the reward \n",
    "            runner.record_step(reward)\n",
    "            \n",
    "            #  we don't start learning until the memory is half full\n",
    "            if step > int(agent.memory.size * 0.5):\n",
    "                train_info = agent.learn()\n",
    "                            \n",
    "        runner.record_episode(env_info=info)\n",
    "        \n",
    "        if episode % 10 == 1:\n",
    "            print('ep {} {:.2f} % rew {}'.format(episode, 100 * step / TOTAL_STEPS, runner.episode_rewards[-1]))\n",
    "            \n",
    "print_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  process the last episode into a pd.DataFrame\n",
    "ep_hist = energy_py.experiments.process_env_info(env, info)\n",
    "ep_hist.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_hist.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#  plot the last episode\n",
    "f = energy_py.experiments.plot_battery_episode(ep_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot learning progress\n",
    "f = energy_py.experiments.plot_time_series(\n",
    "    pd.DataFrame(\n",
    "        np.array(np.array(runner.episode_rewards).reshape(-1, 1)),\n",
    "        columns=['total_reward']\n",
    "    ),\n",
    "    'total_reward'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
