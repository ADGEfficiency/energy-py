{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with Battery Example\n",
    "This notebook demonstrates the ability of a DQN agent to learn to optimize electric battery storage.\n",
    "\n",
    "This example involves a constant and repetitive electricity price profile, combined with a perfect forecast. The agent has both the ability to memorize this profile and lives in a near Markov environment.  More interesting work randomly samples different price rollouts and uses realistic forecasts.  \n",
    "\n",
    "A real world application of using reinforcement learning to control a battery would have to deal with both a variable price profile and a non-Markov understanding of what the price profile would do in the future.  It could also involve additional reward signals, such as payments from fast frequency response needed to be balanced against price arbitrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/anaconda3/envs/energy_py/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/adam/anaconda3/envs/energy_py/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import energy_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  define a total number of steps for the experiment to run\n",
    "TOTAL_STEPS = 400\n",
    "\n",
    "#  to setup the agent we use a dictionary\n",
    "#  a dictionary allows us to eaisly save the config to csv if we want\n",
    "agent_config = {\n",
    "    'discount': 0.97,                 #  the discount rate\n",
    "    'tau': 0.001,                     #  parameter that controls the copying of weights from online to target network\n",
    "    'total_steps': TOTAL_STEPS,   \n",
    "    'batch_size': 32,                 #  size of the minibatches used for learning\n",
    "    'layers': (50, 50),               #  structure of the neural network used to approximate Q(s,a)\n",
    "    'learning_rate': 0.0001,          #  controls the stength of weight updates during learning       \n",
    "    'epsilon_decay_fraction': 0.3,    #  a fraction as % of total steps where epsilon decayed from 1.0 to 0.1\n",
    "    'memory_fraction': 0.4,           #  the size of the replay memory as a % of total steps\n",
    "    'memory_type': 'deque',           #  the replay memory implementation we want\n",
    "               }\n",
    "\n",
    "#  keep all of the BatteryEnv variables (episode length, efficiency etc) at their defaults\n",
    "#  we just need to let our env know where our state.csv and observation.csv are (data_path)\n",
    "env = energy_py.make_env('battery')\n",
    "\n",
    "#  set seeds for reproducibility\n",
    "env.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "energy_py has code to generate log files and write data to TensorBoard.  \n",
    "\n",
    "For the scope of this low level example, we will just use data available locally - episode rewards in the `Runner` class and data for the last episode - the `info` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/adam/anaconda3/envs/energy_py/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/adam/anaconda3/envs/energy_py/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1 0.50 % rew [[-740.70977193]]\n",
      "ep 2 1.01 % rew [[-732.54714912]]\n",
      "ep 3 1.51 % rew [[-824.32430702]]\n",
      "ep 4 2.02 % rew [[-741.88133333]]\n",
      "ep 5 2.52 % rew [[-602.31462281]]\n",
      "ep 6 3.02 % rew [[-746.74367723]]\n",
      "ep 7 3.53 % rew [[-790.64576316]]\n",
      "ep 8 4.03 % rew [[-988.66138596]]\n",
      "ep 9 4.53 % rew [[-1075.69805466]]\n",
      "ep 10 5.04 % rew [[-728.50772807]]\n",
      "ep 11 5.54 % rew [[-1087.76060526]]\n",
      "ep 12 6.04 % rew [[-704.80015789]]\n",
      "ep 13 6.55 % rew [[-984.60466965]]\n",
      "ep 14 7.05 % rew [[-686.63611671]]\n",
      "ep 15 7.56 % rew [[-682.73321976]]\n",
      "ep 16 8.06 % rew [[-547.50131579]]\n",
      "ep 17 8.56 % rew [[-638.45744737]]\n",
      "ep 18 9.07 % rew [[-940.51618421]]\n",
      "ep 19 9.57 % rew [[-865.81405263]]\n",
      "ep 20 10.07 % rew [[-863.58212281]]\n",
      "ep 21 10.58 % rew [[-708.23535088]]\n",
      "ep 22 11.08 % rew [[-884.05858772]]\n",
      "ep 23 11.59 % rew [[-943.33284211]]\n",
      "ep 24 12.09 % rew [[-760.93062503]]\n",
      "ep 25 12.59 % rew [[-749.26624561]]\n",
      "ep 26 13.10 % rew [[-749.08092982]]\n",
      "ep 27 13.60 % rew [[-911.14857895]]\n",
      "ep 28 14.11 % rew [[-655.94108772]]\n",
      "ep 29 14.61 % rew [[-1049.96595614]]\n",
      "ep 30 15.11 % rew [[-1112.09575439]]\n",
      "ep 31 15.62 % rew [[-827.47776316]]\n",
      "ep 32 16.12 % rew [[-742.37183333]]\n",
      "ep 33 16.62 % rew [[-847.40864912]]\n",
      "ep 34 17.13 % rew [[-734.60403509]]\n",
      "ep 35 17.63 % rew [[-896.39035965]]\n",
      "ep 36 18.14 % rew [[-638.72295614]]\n",
      "ep 37 18.64 % rew [[-761.23953509]]\n",
      "ep 38 19.14 % rew [[-933.7012807]]\n",
      "ep 39 19.65 % rew [[-782.49014035]]\n",
      "ep 40 20.15 % rew [[-677.05579825]]\n",
      "ep 41 20.65 % rew [[-694.82146491]]\n",
      "ep 42 21.16 % rew [[-825.74178947]]\n",
      "ep 43 21.66 % rew [[-622.81663158]]\n",
      "ep 44 22.16 % rew [[-807.153]]\n",
      "ep 45 22.67 % rew [[-817.07099123]]\n",
      "ep 46 23.17 % rew [[-429.42585965]]\n",
      "ep 47 23.68 % rew [[-979.61435965]]\n",
      "ep 48 24.18 % rew [[-608.47041228]]\n",
      "ep 49 24.68 % rew [[-750.99147368]]\n",
      "ep 50 25.19 % rew [[-412.74887719]]\n",
      "ep 51 25.69 % rew [[-501.44466667]]\n",
      "ep 52 26.20 % rew [[-594.97796491]]\n",
      "ep 53 26.70 % rew [[-469.12372807]]\n",
      "ep 54 27.20 % rew [[-445.52585965]]\n",
      "ep 55 27.71 % rew [[-515.69080702]]\n",
      "ep 56 28.21 % rew [[-626.71707018]]\n",
      "ep 57 28.71 % rew [[-412.65418421]]\n",
      "ep 58 29.22 % rew [[-474.51783333]]\n",
      "ep 59 29.72 % rew [[-423.4167193]]\n",
      "ep 60 30.23 % rew [[-556.27097368]]\n",
      "ep 61 30.73 % rew [[-507.2257193]]\n",
      "ep 62 31.23 % rew [[-431.49946491]]\n",
      "ep 63 31.74 % rew [[-475.97260526]]\n",
      "ep 64 32.24 % rew [[-442.21031579]]\n",
      "ep 65 32.74 % rew [[-388.18905263]]\n",
      "ep 66 33.25 % rew [[-307.08939474]]\n",
      "ep 67 33.75 % rew [[-334.66965789]]\n",
      "ep 68 34.26 % rew [[-385.49185088]]\n",
      "ep 69 34.76 % rew [[-346.43588596]]\n",
      "ep 70 35.26 % rew [[-286.31233333]]\n",
      "ep 71 35.77 % rew [[-364.13050877]]\n",
      "ep 72 36.27 % rew [[-275.47295614]]\n",
      "ep 73 36.77 % rew [[-310.25115789]]\n",
      "ep 74 37.28 % rew [[-186.48890351]]\n",
      "ep 75 37.78 % rew [[-373.21124561]]\n",
      "ep 76 38.28 % rew [[-217.82187719]]\n",
      "ep 77 38.79 % rew [[-234.73463158]]\n",
      "ep 78 39.29 % rew [[-166.27212281]]\n",
      "ep 79 39.80 % rew [[-143.28253509]]\n",
      "ep 80 40.30 % rew [[-220.63740351]]\n",
      "ep 81 40.80 % rew [[-101.38588596]]\n",
      "ep 82 41.31 % rew [[-124.21734211]]\n",
      "ep 83 41.81 % rew [[-148.91405263]]\n",
      "ep 84 42.31 % rew [[-54.41266667]]\n",
      "ep 85 42.82 % rew [[-164.09798246]]\n",
      "ep 86 43.32 % rew [[-30.22032456]]\n"
     ]
    }
   ],
   "source": [
    "#  initialize Tensorflow machinery\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #  Runner is a class that helps us with experiments - tracking rewards, writing environment info to csv and managing TensorBoard\n",
    "    #  in this notebook we just use it to track rewards for us\n",
    "    runner = energy_py.Runner(\n",
    "        sess,  \n",
    "        {'tb_rl': './tb_rl',\n",
    "         'ep_rewards': './rewards.csv'},\n",
    "        log_freq=10\n",
    "    )\n",
    "    \n",
    "    #  add the tf session and the environment to the agent config dictionary\n",
    "    #  and initialize the agent\n",
    "    agent_config['sess'] = sess\n",
    "    agent_config['env'] = env\n",
    "    agent = energy_py.make_agent(agent_id='dqn', **agent_config)\n",
    "    \n",
    "    #  initial values for the step and episode number\n",
    "    step, episode = 0, 0\n",
    "\n",
    "    #  outer while loop runs through multiple episodes\n",
    "    while step < TOTAL_STEPS:\n",
    "        episode += 1\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        \n",
    "        #  inner while loop runs through a single episode\n",
    "        while not done:\n",
    "            step += 1\n",
    "            #  select an action\n",
    "            action = agent.act(observation)\n",
    "            \n",
    "            #  take one step through the environment\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            #  store the experience\n",
    "            agent.remember(observation, action, reward,\n",
    "                           next_observation, done)\n",
    "            \n",
    "            #  moving to the next time step\n",
    "            observation = next_observation\n",
    "            #  saving the reward \n",
    "            runner.record_step(reward)\n",
    "            \n",
    "            #  we don't start learning until the memory is half full\n",
    "            if step > int(agent.memory.size * 0.5):\n",
    "                train_info = agent.learn()\n",
    "            \n",
    "        runner.record_episode(env_info=info)\n",
    "        \n",
    "        if episode %% 5 == 0:\n",
    "            print('ep {} {:.2f} % rew {}'.format(episode, 100 * step / TOTAL_STEPS, runner.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  process the last episode into a pd.DataFrame\n",
    "ep_hist = energy_py.experiments.process_episode(info)\n",
    "ep_hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot the last episode\n",
    "energy_py.experiments.plot_battery_episode(ep_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot learning progress\n",
    "energy_py.experiments.plot_time_series(\n",
    "    runner.episode_rewards,\n",
    "    'total_reward'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
