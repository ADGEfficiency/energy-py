{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning with Battery Example\n",
    "Purpose of this notebook is to demonstrate the ability of a reinforcement learning agent based on Q-Learning to learn to control a battery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from energy_py import EternityVisualizer\n",
    "from energy_py.agents import DQN, Q_DQN\n",
    "from energy_py.envs import BatteryEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  set random seeds for repeatability\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  let our environment know where our state & observation data is\n",
    "data_path = os.getcwd()\n",
    "\n",
    "#  keep all of the BatteryEnv variables (episode length, efficiency etc)\n",
    "#  at their defaults\n",
    "env = BatteryEnv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  define a batch size, discount rate and total number of episodes \n",
    "BATCH_SIZE = 32\n",
    "DISCOUNT = 0.99\n",
    "EPISODES = 10000\n",
    "\n",
    "#  in order to setup hyperparameters like epsilon decay or target net\n",
    "#  update frequency, we need to let our agent know how many total steps\n",
    "#  it will take in it's life\n",
    "total_steps = env.episode_length * EPISODES\n",
    "\n",
    "#  now we setup our agent\n",
    "#  we pass in an object to approximate Q(s,a)\n",
    "#  this object is an energy_py function approximator that uses \n",
    "#  Tensorflow to estimate expected discounted return for each action\n",
    "agent = DQN(env,\n",
    "            discount=DISCOUNT,\n",
    "            Q=Q_DQN,\n",
    "            total_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  simple class to time the experiment\n",
    "\n",
    "class Timer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def calc_time(self, episode, avg_reward):\n",
    "        run_time = time.time() - self.start_time\n",
    "        avg_time = run_time / episode\n",
    "        print('{:1.0f} episodes in {:2.0f} min - avg {:.2f} sec per episode - avg lifetime reward {:3.3f} $/5min'.format(episode, run_time / 60, avg_time, avg_reward))\n",
    "        return run_time, avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0 episodes in  0 min - avg 0.03 sec per episode - avg lifetime reward -0.416 $/5min\n",
      "400.0 episodes in  0 min - avg 0.03 sec per episode - avg lifetime reward -0.423 $/5min\n",
      "600.0 episodes in  0 min - avg 0.04 sec per episode - avg lifetime reward -0.448 $/5min\n",
      "800.0 episodes in  1 min - avg 0.04 sec per episode - avg lifetime reward -0.414 $/5min\n",
      "1000.0 episodes in  1 min - avg 0.05 sec per episode - avg lifetime reward -0.414 $/5min\n",
      "1200.0 episodes in  2 min - avg 0.10 sec per episode - avg lifetime reward -0.399 $/5min\n",
      "1400.0 episodes in  3 min - avg 0.13 sec per episode - avg lifetime reward -0.352 $/5min\n",
      "1600.0 episodes in  4 min - avg 0.17 sec per episode - avg lifetime reward -0.310 $/5min\n",
      "1800.0 episodes in  6 min - avg 0.20 sec per episode - avg lifetime reward -0.251 $/5min\n",
      "2000.0 episodes in  8 min - avg 0.23 sec per episode - avg lifetime reward -0.189 $/5min\n",
      "2200.0 episodes in  9 min - avg 0.25 sec per episode - avg lifetime reward -0.122 $/5min\n",
      "2400.0 episodes in 11 min - avg 0.28 sec per episode - avg lifetime reward -0.051 $/5min\n",
      "2600.0 episodes in 142 min - avg 3.27 sec per episode - avg lifetime reward 0.014 $/5min\n",
      "2800.0 episodes in 317 min - avg 6.80 sec per episode - avg lifetime reward 0.080 $/5min\n",
      "3000.0 episodes in 438 min - avg 8.77 sec per episode - avg lifetime reward 0.137 $/5min\n",
      "3200.0 episodes in 441 min - avg 8.26 sec per episode - avg lifetime reward 0.204 $/5min\n",
      "3400.0 episodes in 443 min - avg 7.82 sec per episode - avg lifetime reward 0.261 $/5min\n",
      "3600.0 episodes in 446 min - avg 7.43 sec per episode - avg lifetime reward 0.314 $/5min\n",
      "3800.0 episodes in 455 min - avg 7.19 sec per episode - avg lifetime reward 0.374 $/5min\n",
      "4000.0 episodes in 506 min - avg 7.59 sec per episode - avg lifetime reward 0.437 $/5min\n",
      "4200.0 episodes in 509 min - avg 7.28 sec per episode - avg lifetime reward 0.496 $/5min\n",
      "4400.0 episodes in 513 min - avg 6.99 sec per episode - avg lifetime reward 0.543 $/5min\n",
      "4600.0 episodes in 564 min - avg 7.36 sec per episode - avg lifetime reward 0.596 $/5min\n",
      "4800.0 episodes in 568 min - avg 7.10 sec per episode - avg lifetime reward 0.645 $/5min\n",
      "5000.0 episodes in 572 min - avg 6.86 sec per episode - avg lifetime reward 0.693 $/5min\n",
      "5200.0 episodes in 626 min - avg 7.23 sec per episode - avg lifetime reward 0.743 $/5min\n",
      "5400.0 episodes in 630 min - avg 7.00 sec per episode - avg lifetime reward 0.788 $/5min\n",
      "5600.0 episodes in 637 min - avg 6.82 sec per episode - avg lifetime reward 0.843 $/5min\n",
      "5800.0 episodes in 642 min - avg 6.64 sec per episode - avg lifetime reward 0.879 $/5min\n",
      "6000.0 episodes in 647 min - avg 6.47 sec per episode - avg lifetime reward 0.925 $/5min\n",
      "6200.0 episodes in 653 min - avg 6.32 sec per episode - avg lifetime reward 0.969 $/5min\n",
      "6400.0 episodes in 659 min - avg 6.18 sec per episode - avg lifetime reward 1.013 $/5min\n",
      "6600.0 episodes in 668 min - avg 6.07 sec per episode - avg lifetime reward 1.051 $/5min\n",
      "6800.0 episodes in 695 min - avg 6.13 sec per episode - avg lifetime reward 1.075 $/5min\n",
      "7000.0 episodes in 726 min - avg 6.22 sec per episode - avg lifetime reward 1.108 $/5min\n",
      "7200.0 episodes in 757 min - avg 6.31 sec per episode - avg lifetime reward 1.139 $/5min\n",
      "7400.0 episodes in 791 min - avg 6.42 sec per episode - avg lifetime reward 1.177 $/5min\n",
      "7600.0 episodes in 831 min - avg 6.56 sec per episode - avg lifetime reward 1.207 $/5min\n",
      "7800.0 episodes in 878 min - avg 6.75 sec per episode - avg lifetime reward 1.236 $/5min\n",
      "8000.0 episodes in 929 min - avg 6.96 sec per episode - avg lifetime reward 1.257 $/5min\n",
      "8200.0 episodes in 981 min - avg 7.18 sec per episode - avg lifetime reward 1.280 $/5min\n",
      "8400.0 episodes in 1040 min - avg 7.43 sec per episode - avg lifetime reward 1.304 $/5min\n"
     ]
    }
   ],
   "source": [
    "#  we track total steps to know when to learn and \n",
    "#  update the target net\n",
    "total_step = 0\n",
    "timer = Timer()\n",
    "\n",
    "#  initialize Tensorflow machinery\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #  for loop over episodes\n",
    "    for episode in range(1, EPISODES):\n",
    "        #  initialize before starting episode\n",
    "        done, step = False, 0\n",
    "        #  reset the environment\n",
    "        observation = env.reset(episode)\n",
    "        \n",
    "        #  a while loop to run through a single episode\n",
    "        #  will terminate when the env returns done=True after a step\n",
    "        while done is False:\n",
    "            #  agent uses observation to select an action\n",
    "            action = agent.act(sess=sess, obs=observation)\n",
    "            \n",
    "            #  take a step through the environment\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            #  store the experience in the agent memory\n",
    "            agent.memory.add_experience(observation, action, reward,\n",
    "                                       next_observation, done,\n",
    "                                       step, episode)\n",
    "            \n",
    "            #  the DQN agent has an initial number of steps of\n",
    "            #  no learning to fill it's memory\n",
    "            #  if we are beyond this we learn at each step\n",
    "            if total_step > agent.initial_random:\n",
    "                #  get a batch of experience\n",
    "                #  this is naive experience replay (no prioritization)\n",
    "                batch = agent.memory.get_random_batch(BATCH_SIZE)\n",
    "                \n",
    "                #  learn using the batch\n",
    "                training_info = agent.learn(sess=sess, batch=batch)\n",
    "                \n",
    "                #  optionally update the target network\n",
    "                if total_step % agent.update_target_net == 0:\n",
    "                    agent.update_target_network(sess)\n",
    "                    \n",
    "            #  move on to the next step\n",
    "            step += 1\n",
    "            total_step += 1\n",
    "            observation = next_observation\n",
    "            \n",
    "        if episode % 200 == 0:\n",
    "            avg_reward = np.mean(agent.memory.rewards)\n",
    "            run_time, avg_time = timer.calc_time(episode, avg_reward)\n",
    "            \n",
    "#  one final run of the timer\n",
    "run_time, avg_time = timer.calc_time(episode, avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  now our experiment is over, we can look at the results\n",
    "#  create an object to collect data and create figures\n",
    "hist = EternityVisualizer(agent, env)\n",
    "\n",
    "#  implement the functionality of the EternityVisualizer class\n",
    "agent_outputs, env_outputs = hist.output_results(save_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the Eternity Visualizer has created a number of figures from\n",
    "#  the agent and environment\n",
    "hist.figs_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter magic command to get matplotlib to play nice \n",
    "%matplotlib inline  \n",
    "\n",
    "#  the most useful figure is the reward panel, showing:\n",
    "#  1 reward per episode (and max reward seen so far)\n",
    "#  2 the mean total reward of the last 10% of episodes\n",
    "#    shaded = the standard deviation (also last 10% of episodes)\n",
    "hist.figs_dict['reward_panel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we can also take a look at a figure generated by the Battery env\n",
    "#  showing the results of the last episode\n",
    "#  1 - the gross rate of charge/discharge\n",
    "#  2 - the charge level of the battery at the end of each step\n",
    "#  3 - the electricity price\n",
    "hist.figs_dict['env_panel_fig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we can see from the loss function where we update the target \n",
    "#  network parameters, as the loss spikes up\n",
    "hist.figs_dict['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we can have a look at the unscaled training targets - note the range between +15 to -15\n",
    "hist.figs_dict['unscaled_targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  and also the scaled targets, which have been processed by the agent\n",
    "#  using an energy_py Processor object - note the range between 0 and 1\n",
    "hist.figs_dict['scaled_targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we can have a look at the epsilon decay schedule\n",
    "hist.figs_dict['epsilon']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
