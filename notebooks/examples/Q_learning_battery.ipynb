{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning with Battery Example\n",
    "This notebook demonstrates the ability of a DQN agent to learn to optimize electric battery storage.\n",
    "\n",
    "This example involves a constant and repetitive electricity price profile, combined with a perfect forecast. The agent has both the ability to memorize this profile and lives in a near Markov environment.  \n",
    "\n",
    "A real world application of using reinforcement learning to control a battery would have to deal with both a variable price profile and a non-Markov understanding of what the price profile would do in the future.  It could also involve additional reward signals, such as payments from fast frequency response needed to be balanced against price arbitrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import energy_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  set random seeds for repeatability\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  define a total number of steps for the experiment to run\n",
    "TOTAL_STEPS = 400000\n",
    "\n",
    "#  to setup the agent we use a dictionary\n",
    "#  a dictionary allows us to eaisly save the config to csv if we want\n",
    "agent_config = {'discount': 0.97,                 #  the discount rate\n",
    "                'tau': 0.001,                     #  parameter that controls the copying of weights from online to target network\n",
    "                'total_steps': TOTAL_STEPS,   \n",
    "                'batch_size': 32,                 #  size of the minibatches used for learning\n",
    "                'layers': (50, 50),               #  structure of the neural network used to approximate Q(s,a)\n",
    "                'learning_rate': 0.0001,          #  controls the stength of weight updates during learning       \n",
    "                'epsilon_decay_fraction': 0.3,    #Â  a fraction as % of total steps where epsilon decayed from 1.0 to 0.1\n",
    "                'memory_fraction': 0.4,           #  the size of the replay memory as a % of total steps\n",
    "                'memory_type': 'deque',           #  the replay memory implementation we want\n",
    "                'process_observation': 'standardizer',\n",
    "                'process_target': 'normalizer'}    \n",
    "\n",
    "#  keep all of the BatteryEnv variables (episode length, efficiency etc) at their defaults\n",
    "#  we just need to let our env know where our state.csv and observation.csv are (data_path)\n",
    "env = energy_py.make_env('Battery', dataset_name='example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying ['hidden_layer_1', 'Variable:0'] to ['hidden_layer_1', 'Variable:0']\n",
      "copying ['hidden_layer_1', 'Variable_1:0'] to ['hidden_layer_1', 'Variable_1:0']\n",
      "copying ['input_layer', 'Variable:0'] to ['input_layer', 'Variable:0']\n",
      "copying ['input_layer', 'Variable_1:0'] to ['input_layer', 'Variable_1:0']\n",
      "copying ['output_layer', 'Variable:0'] to ['output_layer', 'Variable:0']\n",
      "copying ['output_layer', 'Variable_1:0'] to ['output_layer', 'Variable_1:0']\n",
      "progress - 5.9% - episode 500 - run time 0.45 - episode reward -63.07 - avg rewards -27.56\n",
      "progress - 11.8% - episode 1000 - run time 1.15 - episode reward 35.53 - avg rewards -35.01\n",
      "progress - 17.6% - episode 1500 - run time 2.17 - episode reward -2.72 - avg rewards -44.10\n",
      "progress - 23.5% - episode 2000 - run time 6.93 - episode reward -36.40 - avg rewards 14.01\n",
      "progress - 29.4% - episode 2500 - run time 14.62 - episode reward -43.77 - avg rewards -50.63\n",
      "progress - 35.2% - episode 3000 - run time 25.48 - episode reward -72.94 - avg rewards -48.49\n",
      "progress - 41.1% - episode 3500 - run time 36.95 - episode reward -66.23 - avg rewards -22.99\n",
      "progress - 47.0% - episode 4000 - run time 48.50 - episode reward -26.27 - avg rewards -28.06\n",
      "progress - 52.9% - episode 4500 - run time 60.25 - episode reward 15.79 - avg rewards -3.83\n",
      "progress - 58.8% - episode 5000 - run time 67.35 - episode reward -24.56 - avg rewards -26.75\n"
     ]
    }
   ],
   "source": [
    "#  Runner is a class that helps us with experiments - tracking rewards, writing environment info to csv and managing TensorBoard\n",
    "#  in this notebook we just use it to track rewards for us\n",
    "runner = energy_py.Runner()\n",
    "\n",
    "#  initialize Tensorflow machinery\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #  add the tf session and the environment to the agent config dictionary\n",
    "    #  and initialize the agent\n",
    "    agent_config['sess'] = sess\n",
    "    agent_config['env'] = env\n",
    "    agent = energy_py.make_agent(agent_id='DQN', **agent_config)\n",
    "    \n",
    "    #  initial values for the step and episode number\n",
    "    step, episode = 0, 0\n",
    "\n",
    "    #  outer while loop runs through multiple episodes\n",
    "    while step < TOTAL_STEPS:\n",
    "        episode += 1\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        \n",
    "        #  inner while loop runs through a single episode\n",
    "        while not done:\n",
    "            step += 1\n",
    "            #  select an action\n",
    "            action = agent.act(observation)\n",
    "            \n",
    "            #  take one step through the environment\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            #  store the experience\n",
    "            agent.remember(observation, action, reward,\n",
    "                           next_observation, done)\n",
    "            \n",
    "            #  moving to the next time step\n",
    "            observation = next_observation\n",
    "            #  saving the reward \n",
    "            runner.append(reward)\n",
    "            \n",
    "            #  we don't start learning until the memory is half full\n",
    "            if step > int(agent.memory.size * 0.5):\n",
    "                train_info = agent.learn()\n",
    "        \n",
    "        if episode % 500 == 0:\n",
    "            log = 'progress - {:.1f}% - episode {:.0f} - run time {:.2f} - episode reward {:.2f} - avg rewards {:.2f}'\n",
    "            print(log.format(100 * step / TOTAL_STEPS,\n",
    "                             episode,\n",
    "                             runner.calc_time(),\n",
    "                             sum(runner.ep_rewards),\n",
    "                             runner.avg_rew))\n",
    "            \n",
    "        #  the report method should be run at the end of each episode\n",
    "        runner.report({'ep': episode,\n",
    "                       'step': step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  energy_py uses TensorBoard for logging - for the scope of this notebook example we will do\n",
    "#  some simple plotting using matplotlib\n",
    "episode_rewards = runner.global_rewards\n",
    "plt.plot(episode_rewards, label='Total reward per episode [$]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we can also look at what happened in our last episode\n",
    "ep_hist = pd.DataFrame.from_dict(info)\n",
    "ep_hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ep_hist.loc[:, 'new_charge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ep_hist.loc[:, 'electricity_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
