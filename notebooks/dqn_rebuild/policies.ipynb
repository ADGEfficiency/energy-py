{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for the forward pass across the q-network and for policies\n",
    "\n",
    "Inspired by open ai baselines deepq\n",
    "\n",
    "TODO Bayesian w/ dropout!\n",
    "\n",
    "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf\n",
    "\n",
    "TODO - deep-rl-tensorflow networks (dueling, plus the structure in network.py (Network class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/anaconda3/envs/energy_py/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/adam/anaconda3/envs/energy_py/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/adam/anaconda3/envs/energy_py/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "def feed_forward(hiddens, observation, num_actions, scope, layer_norm=False):\n",
    "    \"\"\"\n",
    "THIS IS JUST A SAMPLE - more developed version is in networks.ipynb\n",
    "    \n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=False):\n",
    "        out = observation\n",
    "        \n",
    "        for hidden in hiddens:\n",
    "            out = layers.fully_connected(out, num_outputs=hidden, activation_fn=None)\n",
    "            \n",
    "            if layer_norm:\n",
    "                out = layers.layer_norm(out, center=True, scale=True)\n",
    "                \n",
    "            layer = tf.nn.relu(out)\n",
    "\n",
    "    return layers.fully_connected(layer, num_outputs=num_actions, activation_fn=None)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = (50, 50)\n",
    "observation = tf.placeholder(shape=(None,2), dtype=tf.float32)\n",
    "num_actions = 3\n",
    "scope = 'online'\n",
    "\n",
    "q_values = feed_forward(hiddens, observation, num_actions, scope, layer_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35826737  0.54035956  0.2509272 ]\n",
      " [-0.15241034  0.7827507   0.1708551 ]\n",
      " [-0.20343663  0.8433749   0.13169017]]\n",
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def greedy_policy(q_values):\n",
    "    return tf.argmax(q_values, axis=1)    \n",
    "    \n",
    "greedy = greedy_policy(q_values)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    q, g = sess.run([q_values, greedy], {observation: np.arange(6).reshape(3, -1)})\n",
    "    print(q)\n",
    "    print(g)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_policy(q_values):\n",
    "    epsilon = tf.placeholder(shape=(), dtype=tf.float32, name='epsilon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 3\n",
    "#  requires an array as input\n",
    "r = tf.random_uniform(tf.stack([bs]),\n",
    "                      minval=0,\n",
    "                      maxval=num_actions,\n",
    "                      dtype=tf.int64)\n",
    "with tf.Session() as sess:\n",
    "    out = sess.run(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 [[-2.8370497  -0.14282192 -2.5970454 ]\n",
      " [-0.82877815 -1.5227816  -1.0633032 ]\n",
      " [-0.78820354 -1.9261627  -0.91720873]] [0.48352242 1.0611279  1.005566  ] [[1]\n",
      " [0]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "def softmax_policy(q_values):\n",
    "    \"\"\"\n",
    "    \n",
    "    Basic structure of the policy from here\n",
    "    https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf\n",
    "    \n",
    "    Calculation of entropy from here\n",
    "    https://github.com/dennybritz/reinforcement-learning/issues/34\n",
    "    \n",
    "    Log of the probabilities are taken for speed, accuracy and stability \n",
    "    \"\"\"\n",
    "    temperature = tf.placeholder(shape=(), dtype=tf.float32, name='softmax_temperature')\n",
    "    softmax = tf.nn.softmax(tf.divide(q_values, temperature), \n",
    "                            axis=1)\n",
    "    log_probs = tf.log(softmax)\n",
    "    \n",
    "    entropy = -tf.reduce_sum(softmax * log_probs, 1, name='softmax_entropy')\n",
    "    \n",
    "    samples = tf.multinomial(log_probs, num_samples=1)\n",
    "    \n",
    "    return temperature, log_probs, entropy, samples\n",
    "\n",
    "temperature, probs, entropy, samples = softmax_policy(q_values)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    t, p, e, s = sess.run([temperature, probs, entropy, samples], \n",
    "                          {observation: np.arange(6).reshape(3, -1), \n",
    "                           temperature: 0.5})\n",
    "    \n",
    "print(t, p, e, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
