{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post shows how to perform two of the required operations for DQN and DDQN\n",
    "- copying online network weights to the target network\n",
    "- sharing weights between an online network that predicts Q(s,a) and an online network that predicts Q(s',a)\n",
    "\n",
    "In DQN we parameterize two neural networks \n",
    "- an online network which is used to select actions via an argmax across all actions\n",
    "- a target network which is used to estimate the value of `Q(s',a)`, the expected discounted return for the next state\n",
    "\n",
    "The online network weights are changed to minimize the temporal difference error\n",
    "`td_error = Q(s,a) - r + gamma Q(s',a)`\n",
    "\n",
    "To implement DQN we need some way to update the target network parameters as our online network changes.  There are two methods for this\n",
    "1 - every C steps, copy the online weights to the target weights\n",
    "2 - at each step, set the target network weights to a weighted combination of the old target weights and the online network weights\n",
    "\n",
    "Below I show how to do both of these in TensorFlow using a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/anaconda3/envs/energy_py/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/adam/anaconda3/envs/energy_py/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DDQN the structure of the Bellman target is different than in DQN.  We use the online network to select the best action in the next state, but use the target network to get the estimate.\n",
    "\n",
    "We want to be able to do the training operation in a single Tensorflow session call (session calls are expensive!).  To do this we need a second online network, that shares weights with our acting online network, but is connected to a different placeholder. \n",
    "\n",
    "Below I show how to share weights between two online networks, and to create a target network that has different weights.  To do this we need to do a few things\n",
    "- use `tf.get_variable` to create weights and biases\n",
    "- create both networks under the same variable scope\n",
    "- call `scope.reuse_variables` in between.  \n",
    "- set `reuse=tf.AUTO_REUSE` in the lowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = tf.placeholder(shape=(None, 5), dtype=tf.float32)\n",
    "next_obs = tf.placeholder(shape=(None, 5), dtype=tf.float32)\n",
    "\n",
    "o_p = np.arange(5).reshape(1, 5)\n",
    "no_p = np.arange(5).reshape(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layer(scope, \n",
    "                          input_tensor, \n",
    "                          input_shape, \n",
    "                          output_nodes,\n",
    "                          activation='relu'):\n",
    "    \"\"\"\n",
    "    Creates a single fully connected layer\n",
    "    \n",
    "    args\n",
    "        scope (str) usually 'input_layer' or 'hidden_layer_2' etc\n",
    "        input_tensor (tensor) \n",
    "        input_shape (tuple or int) \n",
    "        output_nodes (int)\n",
    "        activation (str) currently support relu or linear\n",
    "        \n",
    "    To correctly name the variables and still allow variable sharing:\n",
    "    with tf.name_scope('online_network):\n",
    "        layer = fully_connected_layer('input_layer', ...)\n",
    "        \n",
    "    \"\"\"\n",
    "    #  feed input shape as a tuple for support for high dimensional inputs\n",
    "    if isinstance(input_shape, int):\n",
    "        input_shape = (input_shape,)\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        weights = tf.get_variable(\n",
    "            'weights',\n",
    "            shape=(*input_shape, output_nodes),\n",
    "            initializer=tf.contrib.layers.xavier_initializer()\n",
    "        )\n",
    "\n",
    "        bias = tf.get_variable(\n",
    "            'bias',\n",
    "            shape=(output_nodes),\n",
    "            initializer=tf.zeros_initializer()\n",
    "        )\n",
    "\n",
    "        layer = tf.add(\n",
    "            tf.matmul(input_tensor, weights),\n",
    "            bias,\n",
    "            name='layer'\n",
    "        )\n",
    "        \n",
    "    if activation == 'relu':\n",
    "        return tf.nn.relu(layer)\n",
    "    \n",
    "    elif activation == 'linear':\n",
    "        return layer\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Activation of {} not supported'.format(activation))\n",
    "    \n",
    "def feed_forward_network(scope,input_tensor,\n",
    "                 input_shape,\n",
    "                 hiddens,\n",
    "                 output_nodes):\n",
    "    \"\"\"\n",
    "    Creates a feed forward neural network (aka multilayer perceptron)\n",
    "    \n",
    "    args\n",
    "        input_tensor (tensor)\n",
    "        input_shape (tuple or int)\n",
    "        hiddens (list) has nodes per layer (includes input layer)\n",
    "        output_nodes (int)\n",
    "    \"\"\"\n",
    "    with tf.name_scope(scope):\n",
    "        layer = fully_connected_layer(\n",
    "            'input_layer',\n",
    "            input_tensor,\n",
    "            input_shape,\n",
    "            hiddens[0])\n",
    "\n",
    "        for layer_num, nodes in enumerate(hiddens[1:]):\n",
    "            layer = fully_connected_layer(\n",
    "                'hidden_layer_{}'.format(layer_num),\n",
    "                layer,\n",
    "                (hiddens[layer_num-1],),\n",
    "                nodes\n",
    "            )\n",
    "\n",
    "        output_layer = fully_connected_layer(\n",
    "            'output_layer',\n",
    "            layer,\n",
    "            (hiddens[-1],),\n",
    "            output_nodes,\n",
    "            activation='linear'\n",
    "        )\n",
    "\n",
    "    return output_layer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/adam/anaconda3/envs/energy_py/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "[[0.35389137 0.         4.3206797  0.         0.918682   1.7593181\n",
      "  0.         0.         0.         1.2145033 ]]\n",
      "[[0.35389137 0.         4.3206797  0.         0.918682   1.7593181\n",
      "  0.         0.         0.         1.2145033 ]]\n",
      "[[0.         3.0361607  0.28483748 0.         0.         0.\n",
      "  0.         0.         0.34937274 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('online_networks') as scope:\n",
    "    with tf.name_scope('online_obs'):\n",
    "        online = fully_connected_layer('layer_1', obs, (5,), 10)\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    \n",
    "    with tf.name_scope('online_next_obs'):\n",
    "        online_double_q = fully_connected_layer('layer_1', next_obs, (5,), 10)\n",
    "\n",
    "with tf.name_scope('target'):\n",
    "    target = fully_connected_layer('layer_1', obs, (5,), 10)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    o = sess.run(online, {obs: o_p})\n",
    "    \n",
    "    d = sess.run(online_double_q, {next_obs: no_p})\n",
    "    \n",
    "    t = sess.run(target, {obs: o_p})\n",
    "    \n",
    "print(o)\n",
    "print(d)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  now lets try to create a network\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "obs = tf.placeholder(shape=(None, 5), dtype=tf.float32, name='observation')\n",
    "next_obs = tf.placeholder(shape=(None, 5), dtype=tf.float32, name='next_observation')\n",
    "\n",
    "o_p = np.arange(5).reshape(1, 5)\n",
    "no_p = np.arange(5).reshape(1, 5)\n",
    "\n",
    "with tf.variable_scope('online_networks') as scope:\n",
    "\n",
    "    online_obs = feed_forward_network('online_obs', obs, (5,), (5, 5), 2)\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "\n",
    "    online_next_obs = feed_forward_network('online_next_obs', next_obs, (5,), (5, 5), 2)\n",
    "\n",
    "with tf.variable_scope('target_network') as scope:\n",
    "    target = feed_forward_network('target', next_obs, (5,), (5, 5), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    o = sess.run(online_obs, {obs: o_p})\n",
    "    \n",
    "    d = sess.run(online_next_obs, {next_obs: no_p})\n",
    "    \n",
    "    t = sess.run(target, {next_obs: no_p})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.0789158 , -0.71550477]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.0789158 , -0.71550477]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01619816, 2.4164324 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
